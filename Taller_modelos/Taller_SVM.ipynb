{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s-y8Kil2snGk"
   },
   "source": [
    "# Code Assigment 1\n",
    "\n",
    "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
    "\n",
    "You should:\n",
    "\n",
    "1) Specify which Machine Learning problem are you solving.\n",
    "\n",
    "2) Provide a short summary of the features and the labels you are working on.\n",
    "\n",
    "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
    "\n",
    "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
    "\n",
    "5) Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "6) Provide quantitative evidence for generalization using the provided dataset.\n",
    "\n",
    "\n",
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se van ha resolver dos problemas de clasificación binaria:\n",
    "\n",
    "- **Clasificación de billetes**: dado un vector de características de la imagen de un billete se quiere determinar si el billete es falso o no. Para este problema se aplicó la transformada Wavelet a la imagen ,esta transformada divide la imagen en distintas frecuencias y me retorna una serie de coeficientes, coeficientes de detalle y de aproximación.El dataset tiene un vector de característica conformado por lo siguiente:\n",
    "\n",
    "    - variance of Wavelet Transformed image : dispersion de los coeficientes de la transformada.\n",
    "    - skewness of Wavelet Transformed image : indica la asimetría de la distribución de los coeficientes de la transformada,en este contexto nos puede dar información acerca de los bordes de la imagen.\n",
    "    - curtosis of Wavelet Transformed image : caracteriza la elevación o achatamiento de la distribucion de los coeficientes de la transformada, en este contexto puede caracterizar la textura o rugosidad de la imagen.\n",
    "    - entropy of image : nos dice acerca de la cantidad de información de la imagen,si es cercano a cero la imagen puede ser uniforme o suave, por el contrario entre mayor sea la entropia la imagen tiene más detalles.\n",
    "    \n",
    "    por último el dataset cuenta con una etiqueta binaria que nos indica el valor real de clasificación de la imagen, 0 si es falso el billete y 1 si el billete es verdadero.\n",
    "\n",
    "\n",
    "- **Clasificación ocupación de una oficina**: dado un vector de características de las condiciones de una oficina se quiere determinar si la oficina está ocupada o no. El dataset cuenta con el siguiente vector de características:\n",
    "\n",
    "    - date time year-month-day hour: instante en el cual se tomá la medición.\n",
    "    - Temperature, in Celsius: temperatura de la oficina.\n",
    "    - Relative Humidity, %: cantidad de vapor de agua presente en el aire en comparación con la cantidad máxima que el aire puede contener a una temperatura y presión determinadas.\n",
    "    - Light, in Lux: cantidad de luz visible que llega a una superficie.\n",
    "    - CO2, in ppm: concentración de CO2.\n",
    "    - Humidity Ratio in kgwater-vapor/kg-air: cantidad de vapor de agua presente en el aire.\n",
    "\n",
    "    por último el dataset cuenta con una etiqueta binaria que nos indica si la oficina está vacía o no, 0 si está vacía y 1 si está ocupada."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usará SVM como herramienta para solucionar el problema de clasificación, en particular se usará el planteamiento soft-margin usando la Hinge loss function y para optimizar el hiperplano se usará gradiente de descenso. A continuación se presenta el código del que se hace uso para resolver los problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import neccesary libraries to run the code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "5AOO-Ib6o_7U",
    "outputId": "33d76612-b432-4ba1-d8c5-bd2f3e49d706"
   },
   "outputs": [],
   "source": [
    "#reference from: https://towardsdatascience.com/implementing-svm-from-scratch-784e4ad0bc6a\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=1e-3, n_iters=1000):\n",
    "        # initialize the hyperparameters\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        # initialize the model parameters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def _init_weights_bias(self, X):\n",
    "        # initialize the weight vector and bias term to zero\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "    def _get_cls_map(self, y):\n",
    "        # convert the binary labels to a classification map\n",
    "        return np.where(y <= 0, -1, 1)\n",
    "\n",
    "    def _satisfy_constraint(self, x, idx):\n",
    "        # check whether a training example satisfies the margin constraint\n",
    "        linear_model = np.dot(x, self.w) + self.b \n",
    "        return self.cls_map[idx] * linear_model >= 1\n",
    "    \n",
    "    def _get_gradients(self, constrain, x, idx):\n",
    "        # calculate the gradients of the loss function with respect to the model parameters\n",
    "        if constrain:\n",
    "            # if a point satisfies the margin constraint, only the regularization term is used in the gradient calculation\n",
    "            dw = self.w\n",
    "            db = 0\n",
    "            return dw, db\n",
    "        \n",
    "        # if a point does not satisfy the margin constraint, both the regularization and hinge loss terms are used in the gradient calculation\n",
    "        dw = self.w - np.dot(self.cls_map[idx], x)\n",
    "        db = - self.cls_map[idx]\n",
    "        return dw, db\n",
    "    \n",
    "    def _update_weights_bias(self, dw, db):\n",
    "        # update the model parameters using the gradients and the learning rate\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # train the SVM model using gradient descent\n",
    "        self._init_weights_bias(X)\n",
    "        self.cls_map = self._get_cls_map(y)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x in enumerate(X):\n",
    "                # check whether the current training example satisfies the margin constraint\n",
    "                constrain = self._satisfy_constraint(x, idx)\n",
    "                \n",
    "                # calculate the gradients for the current training example\n",
    "                dw, db = self._get_gradients(constrain, x, idx)\n",
    "                \n",
    "                # update the model parameters using the gradients and the learning rate\n",
    "                self._update_weights_bias(dw, db)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # predict the class labels for new examples\n",
    "        estimate = np.dot(X, self.w) + self.b\n",
    "        prediction = np.sign(estimate)\n",
    "        return np.where(prediction == -1, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true==y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para verificar si los datos son linealmente separables se hará uso de la librería sklearn la cual implementa svm y la cual nos retorna las componentes del vector $w$ y el intercepto $b$ que forman el hiperplano que separa los datos en caso de que exista, caso contrario devolveria coeficientes nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# Function to check if tho sets of vectors are linear separables\n",
    "\n",
    "def isLinear(X,Y):\n",
    "    # Create a SVM classifier object with a linear kernel\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    # Train the classifier on the data\n",
    "    clf.fit(X.values, Y.values)\n",
    "\n",
    "    # Comprobar si el clasificador aprendió un hiperplano de separación válido\n",
    "    if clf.coef_ is not None:\n",
    "        # Check if the classifier learned a valid separating hyperplane\n",
    "        print(\"Intercept:\", clf.intercept_)\n",
    "        print(\"Coefficients:\", clf.coef_)\n",
    "        print(\"Los datos son linealmente separables.\")\n",
    "    else:\n",
    "        print(\"Los datos no son linealmente separables.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de billetes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como método para validar la generalización del modelo, y dado que no se poseen más datos, se realizará una partición del dataset original en proporcion 80-20, 80% de los datos se usarán para entrenar el modelo y el otro 20% para ver el comportamiento del modelo con nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "columns_dataset=[\"Variance W.T\",\"Skewness W.T\",\"Curtosis of W.T\",\"Entropy Image\",\"Class\"]\n",
    "df=pd.read_csv('data/bank_note/data_banknote_authentication.txt',names=columns_dataset)\n",
    "\n",
    "#Set of features vectors\n",
    "X=df[[\"Variance W.T\",\"Skewness W.T\",\"Curtosis of W.T\",\"Entropy Image\"]]\n",
    "#Labels\n",
    "y=df[\"Class\"]\n",
    "\n",
    "#Split the data in train and test with a test size of 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a continuación se cuenta el número de muestras que se tiene por cada grupo, obteniendo que hay un número de muestras similar por grupo y de esta manera la muestra en principio representa a cada grupo bajo el supuesto de que la muestra total fue aleatoria y es representativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    762\n",
       "1    610\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of svm.\n",
    "banknote = SVM() \n",
    "\n",
    "#fit the model with the train data\n",
    "banknote.fit(x_train.values,y_train.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.9854545454545455\n"
     ]
    }
   ],
   "source": [
    "#make predictions for the test data\n",
    "predictions = banknote.predict(x_test.values)\n",
    "\n",
    "#Accuracy of the model\n",
    "print(\"SVM Accuracy: \", accuracy(y_test.values, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a lo anterior, se obtuvo que el modelo logró clasificar correctamente el 98.5% de los nuevos datos que tuvo como entrada, lo que nos da indicios de que en principio logró una buena generalización. De acuerdo a esto el número de datos fue suficiente para lograr generalizar, bajo el supuesto de que la muestra es representativa de todos los billetes falsos y reales.\n",
    "\n",
    "Ahora realizaremos la prueba con SVM de sklearn sobre el dataset completo para verificar que en realidad los grupos de datos son linealmente separables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [2.3989271]\n",
      "Coefficients: [[-2.49568987 -1.44322744 -1.73193881 -0.25120478]]\n",
      "Los datos son linealmente separables.\n"
     ]
    }
   ],
   "source": [
    "isLinear(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último a continuación se muestran algunos ejemplos del funcionamiento del modelo, tomando algunos ejemplos aleatorios del dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor real es: 0 \n",
      "el valor de la predicción es: 0\n"
     ]
    }
   ],
   "source": [
    "sample=df.sample()\n",
    "feature_sample=sample[[\"Variance W.T\",\"Skewness W.T\",\"Curtosis of W.T\",\"Entropy Image\"]]\n",
    "real_value=sample[\"Class\"]\n",
    "prediction_value=banknote.predict(feature_sample.values)\n",
    "\n",
    "print(f\"el valor real es: {real_value.iloc[0]} \\nel valor de la predicción es: {prediction_value[0].item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ocupación de oficina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se cuenta con un dataset de entrenamiento y dos datasets de prueba para ver el rendimiento del modelo con datos no vistos con anterioridad.Dado que el atributo $date$ es un string, se decide tomar año,mes,dia,hora y minutos como atributos por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "columns_dataset2=[\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"]\n",
    "train=pd.read_csv('data/occupancy_data/datatraining.txt',names=columns_dataset2,header=0)\n",
    "test1=pd.read_csv('data/occupancy_data/datatest.txt',names=columns_dataset2,header=0)\n",
    "test2=pd.read_csv('data/occupancy_data/datatest2.txt',names=columns_dataset2,header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "#Separate the feature of date in day,month,year,hour,minute\n",
    "train[\"date\"]=pd.to_datetime(train[\"date\"])\n",
    "train['day'] = train[\"date\"].dt.day\n",
    "train['month'] = train[\"date\"].dt.month\n",
    "train['year'] = train[\"date\"].dt.year\n",
    "train['hour'] = train[\"date\"].dt.hour\n",
    "train['minute'] = train[\"date\"].dt.minute\n",
    "#Set of features vectors \n",
    "x_train2=train[[\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\",\"day\",\"month\",\"year\",\"hour\",\"minute\"]]\n",
    "#Labels\n",
    "y_train2=train[\"Occupancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST 1\n",
    "#Separate the feature of date in day,month,year,hour,minute\n",
    "test1[\"date\"]=pd.to_datetime(test1[\"date\"])\n",
    "test1['day'] = test1[\"date\"].dt.day\n",
    "test1['month'] = test1[\"date\"].dt.month\n",
    "test1['year'] = test1[\"date\"].dt.year\n",
    "test1['hour'] = test1[\"date\"].dt.hour\n",
    "test1['minute'] = test1[\"date\"].dt.minute\n",
    "#Set of features vectors \n",
    "x_test1=test1[[\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\",\"day\",\"month\",\"year\",\"hour\",\"minute\"]]\n",
    "#Labels\n",
    "y_test1=test1[\"Occupancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST 2\n",
    "#Separate the feature of date in day,month,year,hour,minute\n",
    "test2[\"date\"]=pd.to_datetime(test2[\"date\"])\n",
    "test2['day'] = test2[\"date\"].dt.day\n",
    "test2['month'] = test2[\"date\"].dt.month\n",
    "test2['year'] = test2[\"date\"].dt.year\n",
    "test2['hour'] = test2[\"date\"].dt.hour\n",
    "test2['minute'] = test2[\"date\"].dt.minute\n",
    "#Set of features vectors \n",
    "x_test2=test2[[\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\",\"day\",\"month\",\"year\",\"hour\",\"minute\"]]\n",
    "#Labels\n",
    "y_test2=test2[\"Occupancy\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la distribución de cada una de las categorías por cada conjunto de datos, con lo que se obtiene que el número de muestras correspondientes a la categoría de la oficina vacía en el dataset de entrenamiento y el segundo dataset de test corresponden alrededor del 78% de la muestra,mientras que en el primer dataset de test corresponde al 63% de la muestra, por lo que en principio sugiere que las muestras del primer dataset de test corresponden a un intervalo de tiempo con un comportamiento distinto en la oficina. Por otra parte teniendo en cuenta que cada dataset corresponde a cierto intervalo de tiempo, el comportamiento en cada intervalo puede variar drasticamente, por lo que tomar solo ciertos intervalos como muestra puede no llegar a representar el comportamiento de la ocupación de la oficina en el día. Lo ideal sería tomar aleatoriamente muestras durante todo el día, toda la semana, todo el mes, y todo el año para lograr una mejor generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra para el conjunto de entrenamiento\n",
      " 0    6414\n",
      "1    1729\n",
      "Name: Occupancy, dtype: int64 \n",
      "\n",
      "Muestra para el primer conjunto de test\n",
      " 0    1693\n",
      "1     972\n",
      "Name: Occupancy, dtype: int64 \n",
      "\n",
      "Muestra para el segundo conjunto de test\n",
      " 0    7703\n",
      "1    2049\n",
      "Name: Occupancy, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Muestra para el conjunto de entrenamiento\\n\",train[\"Occupancy\"].value_counts(),\"\\n\")\n",
    "print(\"Muestra para el primer conjunto de test\\n\",test1[\"Occupancy\"].value_counts(),\"\\n\")\n",
    "print(\"Muestra para el segundo conjunto de test\\n\",test2[\"Occupancy\"].value_counts(),\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación entrenamos nuestro modelo, y a diferencía del punto anterior, se decidió usar un learning_rate mas pequeño con el fin de que pueda encontrar una mejor solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of svm\n",
    "office_ocupancy = SVM(learning_rate=0.00001) \n",
    "\n",
    "#fit the model with the train data\n",
    "office_ocupancy.fit(x_train2.values,y_train2.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.8881801125703565\n"
     ]
    }
   ],
   "source": [
    "#make predictions for the test 1 data\n",
    "predictions = office_ocupancy.predict(x_test1.values)\n",
    "\n",
    "#Accuracy of the model\n",
    "print(\"SVM Accuracy: \", accuracy(y_test1.values, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el primer dataset de prueba se obtuvo una precisión del 88%, con lo cual para los intervalos de tiempo en los que se encuentran las muestras logra tener una buena generalización, sin embargo esto no implíca que haya generalizado por completo, faltarían más datos en intervalos de tiempo distinto o epocas del año distinto en donde las condiciones pueden variar y ahí si observar el rendimiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.8587744074665357\n"
     ]
    }
   ],
   "source": [
    "#make predictions for the test 2 data\n",
    "predictions = office_ocupancy.predict(x_train2.values)\n",
    "\n",
    "#Accuracy of the model\n",
    "print(\"SVM Accuracy: \", accuracy(y_train2.values, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el segundo dataset de prueba se obtuvo una precisión del 86%, similar al anterior dataset de entrenamiento, habría que tener más datos para determinar mejor la generalización del modelo. Por otra parte el rendimiento del modelo puede que se deba a que la muestra se tomó sobre cierto intervalo de tiempo muy reducido, sin embargo las condiciones de la oficina pueden variar drásticamente a lo largo del año debido a cambios clímaticos y demás."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, a continuación se unen los datasets y se determina que efectivamente el dataset es linealmente separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_office_dataset=pd.concat([train,test1,test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_office=all_office_dataset[[\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\",\"day\",\"month\",\"year\",\"hour\",\"minute\"]]\n",
    "all_y_office=all_office_dataset[\"Occupancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-2.38286181]\n",
      "Coefficients: [[-1.61743591e-02  4.72225656e-03  9.01349968e-04  3.11452847e-04\n",
      "  -4.43754755e-05  4.14242710e+00 -4.25415246e-03  1.67865721e-13\n",
      "   1.67347025e-10 -1.98473287e-03  6.44935145e-05]]\n",
      "Los datos son linealmente separables.\n"
     ]
    }
   ],
   "source": [
    "isLinear(all_x_office,all_y_office)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último a continuación se muestran algunos ejemplos del funcionamiento del modelo, tomando algunos ejemplos aleatorios del dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor real es: 0 \n",
      "el valor de la predicción es: 0\n"
     ]
    }
   ],
   "source": [
    "sample_office=test2.sample()\n",
    "feature_sample_office=sample_office[[\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\",\"day\",\"month\",\"year\",\"hour\",\"minute\"]]\n",
    "real_value_office=sample_office[\"Occupancy\"]\n",
    "prediction_value_office=office_ocupancy.predict(feature_sample_office.values)\n",
    "\n",
    "print(f\"el valor real es: {real_value_office.iloc[0]} \\nel valor de la predicción es: {prediction_value_office[0].item()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
